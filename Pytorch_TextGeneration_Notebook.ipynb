{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_TextGeneration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNOXjG1rxzPwXbsW2Lwn5R8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhatHuynhTranSon99/Text-Generation/blob/master/Pytorch_TextGeneration_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tkbSeSVczbp"
      },
      "source": [
        "# Text generation in Andy Weir style using GRU units"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWUpO9qsc7kP"
      },
      "source": [
        "## Library import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiOqrazFc9oR"
      },
      "source": [
        "import torch\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfPneVBQdHuj"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idjbF2U7dLOP"
      },
      "source": [
        "def save_as_pickle(object, file_name):\n",
        "    with open(file_name, \"wb\") as handle:\n",
        "        pickle.dump(object, handle, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "def load_from_pickle(file_name):\n",
        "    with open(file_name, \"rb\") as handle:\n",
        "        result = pickle.load(handle)\n",
        "    return result"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgpPXfA5dUxv"
      },
      "source": [
        "## Create a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgkTgT-rdWk3"
      },
      "source": [
        "class NGramsModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        # Call constructor of parents\n",
        "        super(NGramsModel, self).__init__()\n",
        "\n",
        "        # Cache the sizes\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Create the embedding layer\n",
        "        self.embeddings = torch.nn.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "        # Create gru layer\n",
        "        self.gru = torch.nn.GRU(embedding_size, hidden_size, batch_first=True)\n",
        "\n",
        "        # Create hidden dense layer\n",
        "        self.dense = torch.nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Get the embedding from inputs\n",
        "        embeddings = self.embeddings(inputs)\n",
        "\n",
        "        # Expand embedding by adding an extra dimension\n",
        "        embeddings = embeddings.view(1, embeddings.shape[0], embeddings.shape[1])\n",
        "\n",
        "        # Pass embedding as input into gru and get \n",
        "        # Hidden state for each word in the sentence\n",
        "        hidden_states, _ = self.gru(embeddings)\n",
        "\n",
        "        # Pass h_t into linear layer and get the result\n",
        "        result = self.dense(hidden_states)\n",
        "        return result"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt1hF-lFdbju"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-mxZJGNddXy"
      },
      "source": [
        "# Path of saved pickle files\n",
        "# IMPORTANT: These files can be found in checkpoints folder of the github repo\n",
        "# Put it in the colab folder and then run this cell\n",
        "TOKEN_PATH = \"token.pickle\"\n",
        "WORD2INDEX_PATH = \"word2index.pickle\"\n",
        "NGRAMS_PATH = \"ngrams.pickle\"\n",
        "\n",
        "# Load tokens, word2index mapping and ngrams from pickle files\n",
        "tokens = load_from_pickle(TOKEN_PATH)\n",
        "word2index = load_from_pickle(WORD2INDEX_PATH)\n",
        "ngrams = load_from_pickle(NGRAMS_PATH)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suCn0WvCeBcK"
      },
      "source": [
        "## Training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fx9L2coyNM7",
        "outputId": "948a927e-aa82-408a-a6ce-80561b9a414d"
      },
      "source": [
        "# Create model\n",
        "model = NGramsModel(\n",
        "    vocab_size=len(word2index),\n",
        "    embedding_size=50,\n",
        "    hidden_size=64\n",
        ")\n",
        "\n",
        "# Load in the saved weight\n",
        "model.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "# Create loss and optimizer\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.1, momentum=0.1)\n",
        "\n",
        "# Model to CUDA\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NGramsModel(\n",
              "  (embeddings): Embedding(8136, 50)\n",
              "  (gru): GRU(50, 64, batch_first=True)\n",
              "  (dense): Linear(in_features=64, out_features=8136, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "1lUwMBx7eDcb",
        "outputId": "b12b602f-37f0-4a48-f36d-29527631b6fd"
      },
      "source": [
        "# Start training\n",
        "EPOCHS = 20\n",
        "for epoch in range(EPOCHS):\n",
        "  # Shuffle n grams\n",
        "  random.shuffle(ngrams)\n",
        "\n",
        "  # Save average loss\n",
        "  avg_loss = 0\n",
        "\n",
        "  # Stochastic gradient descent\n",
        "  for i in range(len(ngrams)):\n",
        "    # Display progress\n",
        "    if i % 1000 == 0:\n",
        "      print(f\"Iteration: {i}\")\n",
        "\n",
        "    # Unpack to get inputs and labels\n",
        "    inputs, labels = ngrams[i]\n",
        "    \n",
        "    # Convert to indices\n",
        "    inputs = torch.tensor([word2index[token] for token in inputs], dtype=torch.long)\n",
        "    labels = torch.tensor([word2index[token] for token in labels], dtype=torch.long)\n",
        "\n",
        "    # Inputs and labels to cuda\n",
        "    inputs = inputs.to(\"cuda:0\")\n",
        "    labels = labels.to(\"cuda:0\")\n",
        "\n",
        "    # Reset optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Run through model and get the loss\n",
        "    result = model(inputs)\n",
        "    result = result.view(result.shape[1], result.shape[2]) # Remove the extra dimension by GRU layer\n",
        "    current_loss = loss(result, labels)\n",
        "\n",
        "    # Add to average loss\n",
        "    avg_loss += current_loss.item()\n",
        "\n",
        "    # Perform backprop\n",
        "    current_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # Calculate and display current loss\n",
        "  avg_loss = avg_loss / len(ngrams)\n",
        "  print(f\"Epoch: {epoch + 1}, Average loss: {avg_loss}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-98e6fd993a9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# Shuffle n grams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# Save average loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyYIeT0SgIha"
      },
      "source": [
        "# Save model\n",
        "torch.save(model.state_dict(), \"model.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoxaAoylh0LP"
      },
      "source": [
        "## Generate text using trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX1aN-DXh6jY",
        "outputId": "c66eb401-7f31-4f74-a84d-8cc86909d7f3"
      },
      "source": [
        "# Re-create model from saved weights\n",
        "model = NGramsModel(\n",
        "    vocab_size=len(word2index),\n",
        "    embedding_size=50,\n",
        "    hidden_size=64\n",
        ")\n",
        "\n",
        "# Load in the saved weight\n",
        "# IMPORTANT: This file can be found in the checkpoints folder of github repo\n",
        "# Download it and upload on this colab folder and then run the cell\n",
        "model.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "# Convert model to cuda\n",
        "model.cuda()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NGramsModel(\n",
              "  (embeddings): Embedding(8136, 50)\n",
              "  (gru): GRU(50, 64, batch_first=True)\n",
              "  (dense): Linear(in_features=64, out_features=8136, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-oV2xuYrhgQ"
      },
      "source": [
        "# Create index to word mapping\n",
        "index2word = { v: k for k, v in word2index.items() }"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iPW7avcjwRi"
      },
      "source": [
        "# Softmax\n",
        "def softmax(x):\n",
        "  return np.exp(x) / np.sum(np.exp(x))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQm1IMD8xiOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43e3892e-5a62-402c-a868-beacc9e9082c"
      },
      "source": [
        "# Prediction phase\n",
        "prompt = [\"“\", \"this\", \"is\", \"wrong\"]\n",
        "completed = prompt + []\n",
        "\n",
        "# Run prompt though model\n",
        "with torch.no_grad():\n",
        "  for i in range(80):\n",
        "    # Convert to indices\n",
        "    inputs = torch.tensor([word2index[token] for token in prompt], dtype=torch.long)\n",
        "    inputs = inputs.to(\"cuda:0\")\n",
        "\n",
        "    # Predict next words\n",
        "    result = model(inputs)\n",
        "    result = result.view(result.shape[1], result.shape[2]) # Remove the extra dimension by GRU layer\n",
        "\n",
        "    # Test print\n",
        "    probs = result[-1].cpu().detach().numpy()\n",
        "    prediction = np.random.choice(\n",
        "        a=len(word2index),\n",
        "        p=softmax(probs)\n",
        "    )\n",
        "\n",
        "    completed.append(index2word[prediction])\n",
        "\n",
        "    # Concat to prompt\n",
        "    prompt = prompt[1:]\n",
        "    prompt.append(index2word[prediction])\n",
        "\n",
        "# Print the complete sentence\n",
        "print(\" \".join(completed))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "“ this is wrong . this is that i think where it launches 40 sols before mars . i 'll have a word at different rates . the side is : we do n't the pop tent . lightweight string woven loosely into he ’ ll do n't see what speak : staring at him , blah , blah , ” johanssen smiled . “ actually , ” lewis said . “ watney ? ” lewis asked from the wall . and i told\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJdgkviv5tGr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-YryxtDraJO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}